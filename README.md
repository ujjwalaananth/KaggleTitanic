# KaggleTitanic

Using Extreme Gradient Boosting combined with GridSearchCV algorithm, an accuracy score of 80.382% can be achieved on the popular Kaggle problem Titanic: Machine Learning from Disaster, achieving a rank within the top 11% on the public leaderboard. Implementing this code will require xgboost to be installed. We can use the sklearn wrapper, XGBClassifier() along with GridSearchCV to find the best value of min_child_weight, which was the parameter found to affect the accuracy the most. 
